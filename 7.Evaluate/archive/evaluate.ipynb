{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qid_ranked_docs, qid_rel, k):\n",
    "    \"\"\"\n",
    "    Evaluate. Computes the MRR@k, average nDCG@k, and average precision@k1\n",
    "\n",
    "    Returns:\n",
    "        MRR: float\n",
    "        average_ndcg: float\n",
    "        avg_precision: float\n",
    "        r_pos: int\n",
    "    ----------\n",
    "    Arguments:\n",
    "        qid_ranked_docs: dictionary\n",
    "            key - qid\n",
    "            value - list of cand ans\n",
    "        qid_rel:  dinctionary\n",
    "            key- qid\n",
    "            value - list of relevant ans\n",
    "    \"\"\"\n",
    "    cumulated_reciprocal_rank = 0\n",
    "    num_rel_docs = 0\n",
    "    # Dictionary of the top-k relevancy scores of docs in the candidate answers\n",
    "    rel_scores = {}\n",
    "    precision_list = {}\n",
    "    rank_pos = []\n",
    "\n",
    "    # For each query\n",
    "    for qid in qid_ranked_docs:\n",
    "        # If the query has a relevant passage\n",
    "        if qid in qid_rel:\n",
    "            # Get the list of relevant docs for a query\n",
    "            rel_docs = qid_rel[qid]\n",
    "            # Get the list of ranked docs for a query\n",
    "            cand_docs = qid_ranked_docs[qid]\n",
    "            # Compute relevant scores of the candidates\n",
    "            if qid not in rel_scores:\n",
    "                rel_scores[qid] = []\n",
    "                for i in range(0, k):\n",
    "                    if cand_docs[i] in rel_docs:\n",
    "                        rel_scores[qid].append(1)\n",
    "                    else:\n",
    "                        rel_scores[qid].append(0)\n",
    "            # Compute th reciprocal rank and rank positions\n",
    "            cumulated_reciprocal_rank, r_pos = compute_RR(cand_docs, rel_docs, cumulated_reciprocal_rank, rank_pos, k)\n",
    "\n",
    "    # Compute the average MRR@k across all queries\n",
    "    MRR = cumulated_reciprocal_rank/len(qid_ranked_docs)\n",
    "    # Compute the nDCG@k across all queries\n",
    "    average_ndcg = avg_ndcg(rel_scores, k)\n",
    "\n",
    "    # Compute precision@1\n",
    "    precision_at_k = []\n",
    "    for qid, score in rel_scores.items():\n",
    "        num_rel = 0\n",
    "        for i in range(0, 1):\n",
    "            if score[i] == 1:\n",
    "                num_rel += 1\n",
    "        precision_at_k.append(num_rel/1)\n",
    "\n",
    "    avg_precision = mean(precision_at_k)\n",
    "\n",
    "    return MRR, average_ndcg, avg_precision, r_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, q_text, cands, max_seq_len):\n",
    "    \"\"\"Re-ranks the candidates answers for each question.\n",
    "\n",
    "    Returns:\n",
    "        ranked_ans: list of re-ranked candidate docids\n",
    "        sorted_scores: list of relevancy scores of the answers\n",
    "    -------------------\n",
    "    Arguments:\n",
    "        model - PyTorch model\n",
    "        q_text - str - query\n",
    "        cands -List of retrieved candidate docids\n",
    "        max_seq_len - int\n",
    "    \"\"\"\n",
    "    # Convert list to numpy array\n",
    "    cands_id = np.array(cands)\n",
    "    # Empty list for the probability scores of relevancy\n",
    "    scores = []\n",
    "    # For each answer in the candidates\n",
    "    for docid in cands:\n",
    "        # Map the docid to text\n",
    "        ans_text = docid_to_text[docid]\n",
    "        # Create inputs for the model\n",
    "        encoded_seq = tokenizer.encode_plus(q_text, ans_text,\n",
    "                                            max_length=max_seq_len,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_token_type_ids=True,\n",
    "                                            return_attention_mask = True)\n",
    "\n",
    "        # Numericalized, padded, clipped seq with special tokens\n",
    "        input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
    "        # Specify question seq and answer seq\n",
    "        token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
    "        # Sepecify which position is part of the seq which is padded\n",
    "        att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
    "        # Don't calculate gradients\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions for each QA pair\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
    "        # Get the predictions\n",
    "        logits = outputs[0]\n",
    "        # Apply activation function\n",
    "        pred = softmax(logits, dim=1)\n",
    "        # Move logits and labels to CPU\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        # Append relevant scores to list (where label = 1)\n",
    "        scores.append(pred[:,1][0])\n",
    "        # Get the indices of the sorted similarity scores\n",
    "        sorted_index = np.argsort(scores)[::-1]\n",
    "        # Get the list of docid from the sorted indices\n",
    "        ranked_ans = list(cands_id[sorted_index])\n",
    "        sorted_scores = list(np.around(sorted(scores, reverse=True),decimals=3))\n",
    "\n",
    "    return ranked_ans, sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(model, test_set, max_seq_len):\n",
    "    \"\"\"Re-ranks the candidates answers for each question.\n",
    "\n",
    "    Returns:\n",
    "        qid_pred_rank: Dictionary\n",
    "            key - qid\n",
    "            value - list of re-ranked candidates\n",
    "    -------------------\n",
    "    Arguments:\n",
    "        model - PyTorch model\n",
    "        test_set  List of lists\n",
    "        max_seq_len - int\n",
    "    \"\"\"\n",
    "    # Initiate empty dictionary\n",
    "    qid_pred_rank = {}\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # For each element in the test set\n",
    "    for i, seq in enumerate(tqdm(test_set)):\n",
    "        # question id, list of rel answers, list of candidates\n",
    "        qid, label, cands = seq[0], seq[1], seq[2]\n",
    "        # Map question id to text\n",
    "        q_text = qid_to_text[qid]\n",
    "\n",
    "        # List of re-ranked docids and the corresponding probabilities\n",
    "        ranked_ans, sorted_scores = predict(model, q_text, cands, max_seq_len)\n",
    "\n",
    "        # Dict - key: qid, value: ranked list of docids\n",
    "        qid_pred_rank[qid] = ranked_ans\n",
    "\n",
    "    return qid_pred_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "get_trained_model() downloads a fine-tuned model - the available model_name are:\n",
    "\n",
    "    finbert-qa: 'bert-qa' fine-tuned on FiQA\n",
    "    finbert-domain: 'finbert-domain' fine-tuned on FiQA\n",
    "    finbert-task: 'finberr-task' fine-tuned on FiQA\n",
    "    bert-pointwise: 'bert-base-uncase' fine-tuned on FiQA using the cross-entropy loss\n",
    "    bert-pairwise: 'bert-base-uncase' fine-tuned on FiQA using a pairwise loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-32f3ceb246c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'finbert-qa'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrained_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model/trained/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'finbert-qa'\n",
    "checkpoint = get_trained_model(model_name)\n",
    "\n",
    "trained_model_path = \"model/trained/\" + model_name + \"/\" + checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating:\\n\")\n",
    "# Load model\n",
    "model.load_state_dict(torch.load(trained_model_path))\n",
    "\n",
    "# Get rank\n",
    "qid_pred_rank = get_rank(model, test_set, config['max_seq_len'])\n",
    "\n",
    "k = 10\n",
    "num_q = len(test_set)\n",
    "\n",
    "# Evaluate\n",
    "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, labels, k)\n",
    "\n",
    "print(\"\\n\\nAverage nDCG@{0} for {1} queries: {2:.3f}\".format(k, num_q, average_ndcg))\n",
    "print(\"MRR@{0} for {1} queries: {2:.3f}\".format(k, num_q, MRR))\n",
    "print(\"Average Precision@1 for {0} queries: {1:.3f}\".format(num_q, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
