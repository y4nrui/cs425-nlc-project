{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c1d497-a75b-4283-96a8-ac73b498f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiehy\\OneDrive\\Desktop\\cs425-nlc-project\\9.Query Expansion\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36d7394-207b-4eef-8d69-41af3cbf733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.10.0+cu102)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.9.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.0.19)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: nltk in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yiehy\\appdata\\roaming\\python\\python38\\site-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\yiehy\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2c053-2093-43da-85d9-903b7ff496b4",
   "metadata": {},
   "source": [
    "# Best performing bi-encoder (retrieve and re-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2f34dc-5cbe-4505-b72f-2c902d15d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiehy\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bi_encoder_model = SentenceTransformer(\"../8.Fine-tuned Models/finetuned-bertbase-1epoch\")\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f8af1-cb22-4c54-add4-3deb4ca08ff2",
   "metadata": {},
   "source": [
    "# On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ea7b32-678f-4a12-a9a4-88f76736c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embeddings\n",
    "test_answer_embeddings = pickle.load(open(\"../4.Retrieval/finetuned_bertbase/finetuned_bertbase_test_answer_embeddings.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece7b91b-a862-4a96-86fe-2b71eda49f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../0.Datasets/train_test_split/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74876466-f0d6-4661-9768-a0b780523321",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_index_map = {}\n",
    "for _,row in test_df.iterrows():\n",
    "    if row[\"qid\"] not in question_answer_index_map:\n",
    "        question_answer_index_map[row[\"qid\"]]= []\n",
    "        question_answer_index_map[row[\"qid\"]].append(row[\"docid\"])\n",
    "    else:\n",
    "        question_answer_index_map[row[\"qid\"]].append(row[\"docid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8866eb4-2e4b-4f51-9ef8-9a79edda0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for v in question_answer_index_map.values():\n",
    "    labels.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140b0a02-e783-4197-9c71-ffe931556f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_map = {}\n",
    "label_map = {}\n",
    "for _,row in test_df.iterrows():\n",
    "    if row[\"qid\"] not in question_map:\n",
    "        question_map[row[\"qid\"]] = row[\"question\"]\n",
    "    if row[\"answer\"] not in label_map:\n",
    "        label_map[row[\"answer\"]] = row[\"docid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11d3d219-78ca-4416-ac07-1c300b808475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs basic data cleaning on query: standardize capitalization to lower, remove punctuations, remove redundant whitespaces\n",
    "def basic_cleaning(query):\n",
    "    query = str(query)\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]','',query)\n",
    "    query = ' '.join(query.split())\n",
    "    return query\n",
    "\n",
    "# Extract nouns\n",
    "def nouns_only(query):\n",
    "    try:\n",
    "        tagged_text = nltk.tag.pos_tag(query.split())\n",
    "        nouns_list = [word for word,tag in tagged_text if  tag == 'NNP' or tag == 'NNPS' or tag==\"NN\" or tag==\"NNS\"]\n",
    "        return list(set(nouns_list))\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def query_noun_mapping(query_nouns):\n",
    "    synonym_dict = {}\n",
    "    for query_noun in query_nouns:\n",
    "        try:\n",
    "            closest_noun = wn.synsets(query_noun)[0].lemmas()[1].name()\n",
    "            synonym_dict[query_noun] = closest_noun\n",
    "        except:\n",
    "            pass\n",
    "    return synonym_dict\n",
    "\n",
    "def query_expansion(query):\n",
    "    try:\n",
    "        clean_query = basic_cleaning(query)\n",
    "        query_nouns_list = nouns_only(clean_query)\n",
    "        if len(query_nouns_list)==0:\n",
    "            return query\n",
    "        else:\n",
    "            synonym_dict = query_noun_mapping(query_nouns_list)\n",
    "        if len(synonym_dict.keys()) == 0:\n",
    "            return query\n",
    "        else:\n",
    "            for k,v in synonym_dict.items():\n",
    "                idx = query.lower().index(k)\n",
    "                query = query[:idx] + query[idx:] + f\" and {v}\"\n",
    "        return query\n",
    "    except:\n",
    "        return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "981f0786-c209-4850-931f-a72df1b77c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "test_answer_list = test_df[\"answer\"].tolist()\n",
    "predictions = []\n",
    "count=1\n",
    "for k,v in question_map.items():\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "    try:\n",
    "        v = query_expansion(v)\n",
    "    except:   \n",
    "        pass\n",
    "    question_embedding = bi_encoder_model.encode(query_expansion(v))\n",
    "    answer_similiarity = {}\n",
    "    for i,embed in enumerate(test_answer_embeddings):\n",
    "        answer_similiarity[i]= np.dot(question_embedding, embed)\n",
    "    answer_similiarity = {k: v for k, v in sorted(answer_similiarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_20_hits = []\n",
    "    for item in list(answer_similiarity)[:20]:\n",
    "        top_20_hits.append(test_answer_list[item])\n",
    "    cross_encoder_answer_similiarity = {}\n",
    "    for hit in top_20_hits:\n",
    "        cross_encoder_answer_similiarity[hit] = cross_encoder_model.predict([v,hit])\n",
    "    cross_encoder_answer_similiarity = {k: v for k, v in sorted(cross_encoder_answer_similiarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    label_index = label_map[list(cross_encoder_answer_similiarity.keys())[0]]\n",
    "    predictions.append([label_index])\n",
    "    count+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c343b144-226e-4334-9d22-5620536d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prediction and results\n",
    "results = {\"labels\":labels,\"predictions\":predictions}\n",
    "with open(\"../7.Evaluate/query_expansion.pkl\", 'wb') as f:\n",
    "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970e4ad-b39d-4c61-87a9-8c6fda9840d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf0db1-b89e-4090-8a2a-6db580af1891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b99e7161-5389-4c3d-bd96-87e1dd0f138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs basic data cleaning on query: standardize capitalization to lower, remove punctuations, remove redundant whitespaces\n",
    "def basic_cleaning(query):\n",
    "    query = str(query)\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]','',query)\n",
    "    query = ' '.join(query.split())\n",
    "    return query\n",
    "\n",
    "# Extract nouns\n",
    "def nouns_only(query):\n",
    "    try:\n",
    "        tagged_text = nltk.tag.pos_tag(query.split())\n",
    "        nouns_list = [word for word,tag in tagged_text if  tag == 'NNP' or tag == 'NNPS' or tag==\"NN\" or tag==\"NNS\"]\n",
    "        return list(set(nouns_list))\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def query_noun_mapping(query_nouns):\n",
    "    synonym_dict = {}\n",
    "    for query_noun in query_nouns:\n",
    "        try:\n",
    "            closest_noun = wn.synsets(query_noun)[0].lemmas()[1].name()\n",
    "            synonym_dict[query_noun] = closest_noun\n",
    "        except:\n",
    "            pass\n",
    "    return synonym_dict\n",
    "\n",
    "def query_expansion(query):\n",
    "    try:\n",
    "        clean_query = basic_cleaning(query)\n",
    "        query_nouns_list = nouns_only(clean_query)\n",
    "        if len(query_nouns_list)==0:\n",
    "            return query\n",
    "        else:\n",
    "            synonym_dict = query_noun_mapping(query_nouns_list)\n",
    "        if len(synonym_dict.keys()) == 0:\n",
    "            return query\n",
    "        else:\n",
    "            for k,v in synonym_dict.items():\n",
    "                new_string = f\"{k} and {v}\"\n",
    "                query = query.replace(k, new_string)\n",
    "        return query\n",
    "    except:\n",
    "        return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507927c9-1e68-40cc-bb69-4e0101e9ebef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
