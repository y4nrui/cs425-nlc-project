{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889c9234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentence-transformers\n",
    "# !pip install spacy-wordnet\n",
    "# !pip install -U spacy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c1d497-a75b-4283-96a8-ac73b498f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n",
      "C:\\Users\\yiehy\\OneDrive\\Desktop\\cs425-nlc-project\\6.Query Expansion\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "print(spacy.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2c053-2093-43da-85d9-903b7ff496b4",
   "metadata": {},
   "source": [
    "# Best performing flow (retrieve and re-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2f34dc-5cbe-4505-b72f-2c902d15d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiehy\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bi_encoder_model = SentenceTransformer(\"../10.Fine-tuned Models/finetuned-bertbase-1epoch\")\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f8af1-cb22-4c54-add4-3deb4ca08ff2",
   "metadata": {},
   "source": [
    "# On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ea7b32-678f-4a12-a9a4-88f76736c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### UNCOMMENT AND RUN THE CODE BELOW IF YOU FACE THE ERROR \"UNSUPPORTED PICKLE PROTOCOL: 5\" WHEN RUNNING THIS CELL #####\n",
    "# #!pip3 install pickle5\n",
    "# import pickle5 as pickle\n",
    "##########################################################################################################################\n",
    "\n",
    "# Read embeddings\n",
    "test_answer_embeddings = pickle.load(open(\"../4.Retrieval/finetuned_bertbase/finetuned_bertbase_test_answer_embeddings.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece7b91b-a862-4a96-86fe-2b71eda49f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13786</td>\n",
       "      <td>7817</td>\n",
       "      <td>31330</td>\n",
       "      <td>Can you have a positive return with a balance ...</td>\n",
       "      <td>Have you owned the stock for longer than 2015?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11485</td>\n",
       "      <td>6304</td>\n",
       "      <td>105557</td>\n",
       "      <td>Oversimplify it for me: the correct order of i...</td>\n",
       "      <td>Great questions -- the fact that you're thinki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12711</td>\n",
       "      <td>7115</td>\n",
       "      <td>43508</td>\n",
       "      <td>Definition of “U.S. source” for US non-residen...</td>\n",
       "      <td>The examples you provide in the question are c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10336</td>\n",
       "      <td>5716</td>\n",
       "      <td>287327</td>\n",
       "      <td>Are car buying services worth it?</td>\n",
       "      <td>I have used car buying services through Costco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15084</td>\n",
       "      <td>9016</td>\n",
       "      <td>580920</td>\n",
       "      <td>What makes a Company's Stock prices go up or d...</td>\n",
       "      <td>Here are some significant factors affect the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>6117</td>\n",
       "      <td>3567</td>\n",
       "      <td>579472</td>\n",
       "      <td>Will getting a new credit card and closing ano...</td>\n",
       "      <td>Several events will always result in a reducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>8218</td>\n",
       "      <td>4446</td>\n",
       "      <td>82194</td>\n",
       "      <td>Why are “random” deposits bad?</td>\n",
       "      <td>Random deposits are a bit like playing the lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>6545</td>\n",
       "      <td>3733</td>\n",
       "      <td>99619</td>\n",
       "      <td>Rental Application Fees</td>\n",
       "      <td>Slightly abbreviated version of the guidance f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>1585</td>\n",
       "      <td>1417</td>\n",
       "      <td>303501</td>\n",
       "      <td>Market Cap lower than Shares Outstanding x Sha...</td>\n",
       "      <td>The definition of market cap is exactly shares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>9115</td>\n",
       "      <td>4962</td>\n",
       "      <td>599925</td>\n",
       "      <td>Net Cash Flows from Selling the Bond and Inves...</td>\n",
       "      <td>Investopedia has a good explanation of the ter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3415 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   qid   docid  \\\n",
       "0          13786  7817   31330   \n",
       "1          11485  6304  105557   \n",
       "2          12711  7115   43508   \n",
       "3          10336  5716  287327   \n",
       "4          15084  9016  580920   \n",
       "...          ...   ...     ...   \n",
       "3410        6117  3567  579472   \n",
       "3411        8218  4446   82194   \n",
       "3412        6545  3733   99619   \n",
       "3413        1585  1417  303501   \n",
       "3414        9115  4962  599925   \n",
       "\n",
       "                                               question  \\\n",
       "0     Can you have a positive return with a balance ...   \n",
       "1     Oversimplify it for me: the correct order of i...   \n",
       "2     Definition of “U.S. source” for US non-residen...   \n",
       "3                     Are car buying services worth it?   \n",
       "4     What makes a Company's Stock prices go up or d...   \n",
       "...                                                 ...   \n",
       "3410  Will getting a new credit card and closing ano...   \n",
       "3411                     Why are “random” deposits bad?   \n",
       "3412                            Rental Application Fees   \n",
       "3413  Market Cap lower than Shares Outstanding x Sha...   \n",
       "3414  Net Cash Flows from Selling the Bond and Inves...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Have you owned the stock for longer than 2015?...  \n",
       "1     Great questions -- the fact that you're thinki...  \n",
       "2     The examples you provide in the question are c...  \n",
       "3     I have used car buying services through Costco...  \n",
       "4     Here are some significant factors affect the c...  \n",
       "...                                                 ...  \n",
       "3410  Several events will always result in a reducti...  \n",
       "3411  Random deposits are a bit like playing the lot...  \n",
       "3412  Slightly abbreviated version of the guidance f...  \n",
       "3413  The definition of market cap is exactly shares...  \n",
       "3414  Investopedia has a good explanation of the ter...  \n",
       "\n",
       "[3415 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../0.Datasets/train_test_split/test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74876466-f0d6-4661-9768-a0b780523321",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_index_map = {}\n",
    "for _,row in test_df.iterrows():\n",
    "    if row[\"qid\"] not in question_answer_index_map:\n",
    "        question_answer_index_map[row[\"qid\"]]= []\n",
    "        question_answer_index_map[row[\"qid\"]].append(row[\"docid\"])\n",
    "    else:\n",
    "        question_answer_index_map[row[\"qid\"]].append(row[\"docid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8866eb4-2e4b-4f51-9ef8-9a79edda0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [] # 2d list, with each value in the list being a list of docids that answer the same qid\n",
    "for v in question_answer_index_map.values():\n",
    "    labels.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140b0a02-e783-4197-9c71-ffe931556f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_map = {} # mapping of qid to question text\n",
    "label_map = {} # mapping of docid to answer text\n",
    "for _,row in test_df.iterrows():\n",
    "    if row[\"qid\"] not in question_map:\n",
    "        question_map[row[\"qid\"]] = row[\"question\"]\n",
    "    if row[\"answer\"] not in label_map:\n",
    "        label_map[row[\"answer\"]] = row[\"docid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35acd2-3501-4731-80f5-c22ae3eceefe",
   "metadata": {},
   "source": [
    "# Original Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d3d219-78ca-4416-ac07-1c300b808475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs basic data cleaning on query: standardize capitalization to lower, remove punctuations, remove redundant whitespaces\n",
    "def basic_cleaning(query):\n",
    "    query = str(query)\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]','',query)\n",
    "    query = ' '.join(query.split())\n",
    "    return query\n",
    "\n",
    "# Extract nouns\n",
    "def nouns_only(query):\n",
    "    try:\n",
    "        tagged_text = nltk.tag.pos_tag(query.split())\n",
    "        nouns_list = [word for word,tag in tagged_text if  tag == 'NNP' or tag == 'NNPS' or tag==\"NN\" or tag==\"NNS\"]\n",
    "        return list(set(nouns_list))\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def query_noun_mapping(query_nouns):\n",
    "    synonym_dict = {}\n",
    "    for query_noun in query_nouns:\n",
    "        try:\n",
    "            closest_noun = wn.synsets(query_noun)[0].lemmas()[1].name()\n",
    "            synonym_dict[query_noun] = closest_noun\n",
    "        except:\n",
    "            pass\n",
    "    return synonym_dict\n",
    "\n",
    "def query_expansion(query):\n",
    "    try:\n",
    "        clean_query = basic_cleaning(query)\n",
    "        query_nouns_list = nouns_only(clean_query)\n",
    "        if len(query_nouns_list)==0:\n",
    "            return query\n",
    "        else:\n",
    "            synonym_dict = query_noun_mapping(query_nouns_list)\n",
    "        if len(synonym_dict.keys()) == 0:\n",
    "            return query\n",
    "        else:\n",
    "            for k,v in synonym_dict.items():\n",
    "                new_string = f\"{k} and {v}\"\n",
    "                query = query.replace(k,new_string)\n",
    "        return query\n",
    "    except:\n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e6f1894-50ea-4da4-8170-78e5cc099f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer_list = test_df[\"answer\"].tolist()\n",
    "predictions = []\n",
    "count = 1\n",
    "for k,v in question_map.items():\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "    try:\n",
    "        v = query_expansion(v)\n",
    "    except:\n",
    "        pass\n",
    "    question_embedding = bi_encoder_model.encode(v)\n",
    "    answer_similiarity = {}\n",
    "    for i,embed in enumerate(test_answer_embeddings):\n",
    "        answer_similiarity[i] = np.dot(question_embedding,embed)\n",
    "    answer_similiarity = {k: v for k, v in sorted(answer_similiarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_20_hits = []\n",
    "    for item in list(answer_similiarity)[:20]:\n",
    "        top_20_hits.append(test_answer_list[item])\n",
    "    cross_encoder_answer_similiarity = {}\n",
    "    for hit in top_20_hits:\n",
    "        cross_encoder_answer_similiarity[hit] = cross_encoder_model.predict([v,hit])\n",
    "    cross_encoder_answer_similiarity = {k: v for k, v in sorted(cross_encoder_answer_similiarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    label_index = label_map[list(cross_encoder_answer_similiarity.keys())[0]]\n",
    "    predictions.append([label_index])\n",
    "    count+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813b798-0f97-4b16-a4f7-6e15c16d781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prediction and results\n",
    "results = {\"labels\":labels,\"predictions\":predictions}\n",
    "with open(\"../7.Evaluate/query_expansion.pkl\", 'wb') as f:\n",
    "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7d089",
   "metadata": {},
   "source": [
    "# New Domain-Based Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2988f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an spacy model (supported models are \"es\", \"en\" and \"pt\") \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Spacy 3.x\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "# Spacy 2.x\n",
    "# self.nlp_en.add_pipe(WordnetAnnotator(self.nlp_en.lang))\n",
    "token = nlp('prices')[0]\n",
    "\n",
    "# wordnet object link spacy token with nltk wordnet interface by giving acces to\n",
    "# synsets and lemmas \n",
    "token._.wordnet.synsets()\n",
    "token._.wordnet.lemmas()\n",
    "\n",
    "# And automatically tags with wordnet domains\n",
    "token._.wordnet.wordnet_domains()\n",
    "\n",
    "# query expansion hyperparameters\n",
    "number_of_domain_lemma = 2\n",
    "domains = ['insurance', 'banking', 'finance', 'money']\n",
    "detected_pos = ['NOUN']\n",
    "\n",
    "def basic_cleaning(query):\n",
    "    #Performs basic data cleaning on query: standardize capitalization to lower, remove punctuations, remove redundant whitespaces\n",
    "    query = str(query)\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]','',query)\n",
    "    query = ' '.join(query.split())\n",
    "    return query\n",
    "\n",
    "def get_similar_words(current_token, synonyms_list, number_of_domain_lemma=number_of_domain_lemma):\n",
    "    result = []\n",
    "    if number_of_domain_lemma > len(synonyms_list):\n",
    "        return synonyms_list\n",
    "    for i in range(number_of_domain_lemma):\n",
    "        synonym = synonyms_list[i]\n",
    "        result.append(synonym)\n",
    "\n",
    "    if current_token not in result:\n",
    "        result[number_of_domain_lemma-1] = current_token\n",
    "\n",
    "    return result\n",
    "\n",
    "def query_expansion_v2(query, domains=domains, number_of_domain_lemma=number_of_domain_lemma, detected_pos=detected_pos):\n",
    "    \"\"\"\n",
    "    @params:\n",
    "        query: the question to be query-expanded\n",
    "        domains: the specific domains to get the synonyms from, eg. some possible_domains are ['finance', 'banking', 'insurance', 'money', 'economy']\n",
    "        number_of_domain_lemma: the number of synonyms you want, including the original word\n",
    "    \"\"\"\n",
    "    enriched_sentence = []\n",
    "    sentence = nlp(query)\n",
    "    # For each token in the sentence\n",
    "    for token in sentence:\n",
    "        # print(token.text, token.pos_)\n",
    "        # We get those synsets within the desired domains\n",
    "        synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
    "\n",
    "        if not synsets: \n",
    "            enriched_sentence.append(token.text)\n",
    "        else:\n",
    "            # If we found a synset in the economy domains and the word is a noun\n",
    "            # we get the variants and add them to the enriched sentence\n",
    "            if token.pos_ in detected_pos:\n",
    "                # lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names()] # 2d list comprehension\n",
    "                lemmas_for_synset = []\n",
    "                for s in synsets:\n",
    "                    # print(s.lemma_names())\n",
    "                    for lemma in s.lemma_names():\n",
    "                        # print(lemma)\n",
    "                        lemmas_for_synset.append(lemma)\n",
    "\n",
    "                synonyms = get_similar_words(token.text, lemmas_for_synset, number_of_domain_lemma)\n",
    "                synonyms = list(dict.fromkeys(synonyms))\n",
    "                enriched_sentence.append('{}'.format(' '.join(synonyms)))\n",
    "            else:\n",
    "                enriched_sentence.append(token.text)\n",
    "\n",
    "    enriched_sentence = \" \".join(enriched_sentence)\n",
    "    enriched_sentence = basic_cleaning(enriched_sentence)\n",
    "    enriched_sentence = enriched_sentence.replace(\"_\",\" \")\n",
    "    return enriched_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9471e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Can you have a positive return with a balance below cost basis?\n",
      "Query Expansion Result: can you have a positive tax return return with a balance below cost monetary value basis\n",
      "\n",
      "Original: Oversimplify it for me: the correct order of investing\n",
      "Query Expansion Result: oversimplify it for me the correct order purchase order of investing\n",
      "\n",
      "Original: Definition of “U.S. source” for US non-resident alien capital gains tax\n",
      "Query Expansion Result: definition of us beginning source for us non resident alien capital working capital gains tax taxation\n",
      "\n",
      "Original: Are car buying services worth it?\n",
      "Query Expansion Result: are car auto buy buying services service worth it\n",
      "\n",
      "Original: What makes a Company's Stock prices go up or down?\n",
      "Query Expansion Result: what makes a company company s stock monetary value prices go up or down\n",
      "\n",
      "Original: Tax deductions on empty property\n",
      "Query Expansion Result: tax tax tax writeoff deductions on empty property belongings\n",
      "\n",
      "Original: I am the sole owner of an LLC. Does it make a difference if I file as an S-Corp or a sole-member LLC?\n",
      "Query Expansion Result: i am the sole owner proprietor of an llc does it make a deviation difference if i file as an s corp or a sole member llc\n",
      "\n",
      "Original: Buy home and leverage roommates, or split rent?\n",
      "Query Expansion Result: buy home and leverage roommates or split rent economic rent\n",
      "\n",
      "Original: Do stock option prices predicate the underlying stock's movement?\n",
      "Query Expansion Result: do stock store option monetary value prices predicate the underlying stock store s motion movement\n",
      "\n",
      "Original: Possible pro-rated division of asset strategies without a prenup?\n",
      "Query Expansion Result: possible pro rated division of asset plus strategies without a prenup\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing query expansion function\n",
    "for i in range(10):\n",
    "    test_query = list(question_map.values())[i]\n",
    "    #test_query = question_map[7817]\n",
    "    test_domain=['insurance', 'banking', 'finance', 'money']\n",
    "    print(\"Original:\", test_query)\n",
    "    print(\"Query Expansion Result:\", query_expansion_v2(test_query))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d75896-d53f-418c-8de4-006ff6e212bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Get predictions with new query expansion code\n",
    "\n",
    "# query expansion hyperparameters\n",
    "number_of_domain_lemma = 2\n",
    "domains = ['insurance', 'banking', 'finance', 'money']\n",
    "detected_pos = ['NOUN',\"PROPN\"]\n",
    "\n",
    "test_answer_list = test_df[\"answer\"].tolist()\n",
    "updated_predictions = []\n",
    "count = 1\n",
    "for k,v in question_map.items():\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "    try:\n",
    "        v = query_expansion_v2(v, domains, number_of_domain_lemma, detected_pos)\n",
    "    except:\n",
    "        pass\n",
    "    question_embedding = bi_encoder_model.encode(v)\n",
    "    answer_similiarity = {}\n",
    "    for i,embed in enumerate(test_answer_embeddings):\n",
    "        answer_similiarity[i] = np.dot(question_embedding,embed)\n",
    "    answer_similiarity = {k: v for k, v in sorted(answer_similiarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_20_hits = []\n",
    "    for item in list(answer_similiarity)[:20]:\n",
    "        top_20_hits.append(test_answer_list[item])\n",
    "    cross_encoder_answer_similiarity = {}\n",
    "    for hit in top_20_hits:\n",
    "        cross_encoder_answer_similiarity[hit] = cross_encoder_model.predict([v,hit])\n",
    "    cross_encoder_answer_similiarity = {k: v for k, v in sorted(cross_encoder_answer_similiarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    label_index = label_map[list(cross_encoder_answer_similiarity.keys())[0]]\n",
    "    updated_predictions.append([label_index])\n",
    "    count+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d95c9a0-84b5-49b4-acc5-bf4b9f51fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prediction and results\n",
    "results = {\"labels\":labels,\"predictions\":updated_predictions}\n",
    "with open(\"../7.Evaluate/updated_query_expansion.pkl\", 'wb') as f:\n",
    "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0acc50-7ff5-4a6c-94f3-3e9d76a83d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab8062-5923-4342-8ada-78212e35ddcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916278c-763a-4a41-8813-a93c6aeb02a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae67c46-1447-4a0e-8a0f-69c13596465a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfbfc7-b630-4b70-97bc-27c1c0a1d11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53170300-d665-4a2e-9e9e-e07ca2a672ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506553d-4a01-47ea-ba4a-daa6b967ad43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa6484-57e5-4fbf-8404-5f8dad7b6618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c74c3-a9c7-4fd4-a01e-cc79252a0d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f88a34-0455-4bb4-a19d-6bae4bf2f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1b465-bbd7-4d3a-8200-554b18bbe455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa80d4-87b9-4e01-b684-001e09027274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5369bf-692c-4817-b9cf-93494f1dfd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c343b144-226e-4334-9d22-5620536d849e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
